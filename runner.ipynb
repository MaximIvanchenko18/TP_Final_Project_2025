{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4845244,"sourceType":"datasetVersion","datasetId":2808179}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\ntorch.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:14:40.507729Z","iopub.execute_input":"2024-12-24T18:14:40.508049Z","iopub.status.idle":"2024-12-24T18:14:43.797273Z","shell.execute_reply.started":"2024-12-24T18:14:40.508022Z","shell.execute_reply":"2024-12-24T18:14:43.796574Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'2.4.1+cu121'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"!pip install open_clip_torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:14:43.798384Z","iopub.execute_input":"2024-12-24T18:14:43.798902Z","iopub.status.idle":"2024-12-24T18:14:48.916014Z","shell.execute_reply.started":"2024-12-24T18:14:43.798876Z","shell.execute_reply":"2024-12-24T18:14:48.914906Z"}},"outputs":[{"name":"stdout","text":"Collecting open_clip_torch\n  Downloading open_clip_torch-2.29.0-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (2.4.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.19.1+cu121)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (2024.9.11)\nCollecting ftfy (from open_clip_torch)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (4.66.5)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.24.7)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.4.5)\nRequirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (1.0.12)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (2024.6.1)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->open_clip_torch) (0.2.13)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (2.32.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->open_clip_torch) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->open_clip_torch) (10.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\nDownloading open_clip_torch-2.29.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy, open_clip_torch\nSuccessfully installed ftfy-6.3.1 open_clip_torch-2.29.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"DATA_PATH = '/kaggle/input/'\nIMAGE_PATH = DATA_PATH + 'flickr30k_images'\ndf = pd.read_csv(DATA_PATH + 'captions.txt', delimiter=\",\")\ndf['comment'] = df['comment'].str.lstrip()\nprint(df.loc[19999, 'comment_number'], df.loc[19999, 'comment'], sep=' | ')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:14:48.918293Z","iopub.execute_input":"2024-12-24T18:14:48.918621Z","iopub.status.idle":"2024-12-24T18:14:49.301183Z","shell.execute_reply.started":"2024-12-24T18:14:48.918599Z","shell.execute_reply":"2024-12-24T18:14:49.300518Z"}},"outputs":[{"name":"stdout","text":"4 | A dog runs across the grass .\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"df.columns = ['image', 'caption_number', 'caption']\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:14:49.302310Z","iopub.execute_input":"2024-12-24T18:14:49.302580Z","iopub.status.idle":"2024-12-24T18:14:49.316516Z","shell.execute_reply.started":"2024-12-24T18:14:49.302559Z","shell.execute_reply":"2024-12-24T18:14:49.315825Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                 image  caption_number  \\\n0       1000092795.jpg               0   \n1       1000092795.jpg               1   \n2       1000092795.jpg               2   \n3       1000092795.jpg               3   \n4       1000092795.jpg               4   \n...                ...             ...   \n158910   998845445.jpg               0   \n158911   998845445.jpg               1   \n158912   998845445.jpg               2   \n158913   998845445.jpg               3   \n158914   998845445.jpg               4   \n\n                                                  caption  \n0       Two young guys with shaggy hair look at their ...  \n1       Two young  White males are outside near many b...  \n2        Two men in green shirts are standing in a yard .  \n3            A man in a blue shirt standing in a garden .  \n4                 Two friends enjoy time spent together .  \n...                                                   ...  \n158910  A man in shorts and a Hawaiian shirt leans ove...  \n158911  A young man hanging over the side of a boat  w...  \n158912  A man is leaning off of the side of a blue and...  \n158913  A man riding a small boat in a harbor  with fo...  \n158914  A man on a moored blue and white boat with hil...  \n\n[158915 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption_number</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000092795.jpg</td>\n      <td>0</td>\n      <td>Two young guys with shaggy hair look at their ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000092795.jpg</td>\n      <td>1</td>\n      <td>Two young  White males are outside near many b...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000092795.jpg</td>\n      <td>2</td>\n      <td>Two men in green shirts are standing in a yard .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000092795.jpg</td>\n      <td>3</td>\n      <td>A man in a blue shirt standing in a garden .</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000092795.jpg</td>\n      <td>4</td>\n      <td>Two friends enjoy time spent together .</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>158910</th>\n      <td>998845445.jpg</td>\n      <td>0</td>\n      <td>A man in shorts and a Hawaiian shirt leans ove...</td>\n    </tr>\n    <tr>\n      <th>158911</th>\n      <td>998845445.jpg</td>\n      <td>1</td>\n      <td>A young man hanging over the side of a boat  w...</td>\n    </tr>\n    <tr>\n      <th>158912</th>\n      <td>998845445.jpg</td>\n      <td>2</td>\n      <td>A man is leaning off of the side of a blue and...</td>\n    </tr>\n    <tr>\n      <th>158913</th>\n      <td>998845445.jpg</td>\n      <td>3</td>\n      <td>A man riding a small boat in a harbor  with fo...</td>\n    </tr>\n    <tr>\n      <th>158914</th>\n      <td>998845445.jpg</td>\n      <td>4</td>\n      <td>A man on a moored blue and white boat with hil...</td>\n    </tr>\n  </tbody>\n</table>\n<p>158915 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from PIL import Image\nimport open_clip\nfrom torch.utils.data import Dataset\nfrom transformers import GPT2Tokenizer\nfrom open_clip.factory import create_model_and_transforms\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:14:49.317215Z","iopub.execute_input":"2024-12-24T18:14:49.317402Z","iopub.status.idle":"2024-12-24T18:14:54.148513Z","shell.execute_reply.started":"2024-12-24T18:14:49.317385Z","shell.execute_reply":"2024-12-24T18:14:54.147580Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Макс\ndf1 = df.iloc[0:80000]\ndf1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:32:31.773992Z","iopub.execute_input":"2024-12-24T18:32:31.774362Z","iopub.status.idle":"2024-12-24T18:32:31.785205Z","shell.execute_reply.started":"2024-12-24T18:32:31.774319Z","shell.execute_reply":"2024-12-24T18:32:31.784366Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                image  caption_number  \\\n0      1000092795.jpg               0   \n1      1000092795.jpg               1   \n2      1000092795.jpg               2   \n3      1000092795.jpg               3   \n4      1000092795.jpg               4   \n...               ...             ...   \n79995  3641999223.jpg               0   \n79996  3641999223.jpg               1   \n79997  3641999223.jpg               2   \n79998  3641999223.jpg               3   \n79999  3641999223.jpg               4   \n\n                                                 caption  \n0      Two young guys with shaggy hair look at their ...  \n1      Two young  White males are outside near many b...  \n2       Two men in green shirts are standing in a yard .  \n3           A man in a blue shirt standing in a garden .  \n4                Two friends enjoy time spent together .  \n...                                                  ...  \n79995  A man playing a sport in a green uniform holdi...  \n79996  A person plays cricket while being watched by ...  \n79997  A man runs after a cricket ball on a grass fie...  \n79998    a person playing cricket in an all green outfit  \n79999               A sports player in a green uniform .  \n\n[80000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption_number</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000092795.jpg</td>\n      <td>0</td>\n      <td>Two young guys with shaggy hair look at their ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000092795.jpg</td>\n      <td>1</td>\n      <td>Two young  White males are outside near many b...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000092795.jpg</td>\n      <td>2</td>\n      <td>Two men in green shirts are standing in a yard .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000092795.jpg</td>\n      <td>3</td>\n      <td>A man in a blue shirt standing in a garden .</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000092795.jpg</td>\n      <td>4</td>\n      <td>Two friends enjoy time spent together .</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>79995</th>\n      <td>3641999223.jpg</td>\n      <td>0</td>\n      <td>A man playing a sport in a green uniform holdi...</td>\n    </tr>\n    <tr>\n      <th>79996</th>\n      <td>3641999223.jpg</td>\n      <td>1</td>\n      <td>A person plays cricket while being watched by ...</td>\n    </tr>\n    <tr>\n      <th>79997</th>\n      <td>3641999223.jpg</td>\n      <td>2</td>\n      <td>A man runs after a cricket ball on a grass fie...</td>\n    </tr>\n    <tr>\n      <th>79998</th>\n      <td>3641999223.jpg</td>\n      <td>3</td>\n      <td>a person playing cricket in an all green outfit</td>\n    </tr>\n    <tr>\n      <th>79999</th>\n      <td>3641999223.jpg</td>\n      <td>4</td>\n      <td>A sports player in a green uniform .</td>\n    </tr>\n  </tbody>\n</table>\n<p>80000 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"# Илюха\n# df2 = df.iloc[80000:]\n# df2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:14:54.187645Z","iopub.execute_input":"2024-12-24T18:14:54.187865Z","iopub.status.idle":"2024-12-24T18:14:54.206195Z","shell.execute_reply.started":"2024-12-24T18:14:54.187847Z","shell.execute_reply":"2024-12-24T18:14:54.205549Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                 image  caption_number  \\\n80000   3642088668.jpg               0   \n80001   3642088668.jpg               1   \n80002   3642088668.jpg               2   \n80003   3642088668.jpg               3   \n80004   3642088668.jpg               4   \n...                ...             ...   \n158910   998845445.jpg               0   \n158911   998845445.jpg               1   \n158912   998845445.jpg               2   \n158913   998845445.jpg               3   \n158914   998845445.jpg               4   \n\n                                                  caption  \n80000   A stewardess on an airplane pushes a cart down...  \n80001   A brunette flight attendant in a red uniform i...  \n80002   A flight attendant is pushing a beverage cart ...  \n80003   An airline stewardess is carefully rolling her...  \n80004   Flight attendant in red pushes drink cart thro...  \n...                                                   ...  \n158910  A man in shorts and a Hawaiian shirt leans ove...  \n158911  A young man hanging over the side of a boat  w...  \n158912  A man is leaning off of the side of a blue and...  \n158913  A man riding a small boat in a harbor  with fo...  \n158914  A man on a moored blue and white boat with hil...  \n\n[78915 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption_number</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>80000</th>\n      <td>3642088668.jpg</td>\n      <td>0</td>\n      <td>A stewardess on an airplane pushes a cart down...</td>\n    </tr>\n    <tr>\n      <th>80001</th>\n      <td>3642088668.jpg</td>\n      <td>1</td>\n      <td>A brunette flight attendant in a red uniform i...</td>\n    </tr>\n    <tr>\n      <th>80002</th>\n      <td>3642088668.jpg</td>\n      <td>2</td>\n      <td>A flight attendant is pushing a beverage cart ...</td>\n    </tr>\n    <tr>\n      <th>80003</th>\n      <td>3642088668.jpg</td>\n      <td>3</td>\n      <td>An airline stewardess is carefully rolling her...</td>\n    </tr>\n    <tr>\n      <th>80004</th>\n      <td>3642088668.jpg</td>\n      <td>4</td>\n      <td>Flight attendant in red pushes drink cart thro...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>158910</th>\n      <td>998845445.jpg</td>\n      <td>0</td>\n      <td>A man in shorts and a Hawaiian shirt leans ove...</td>\n    </tr>\n    <tr>\n      <th>158911</th>\n      <td>998845445.jpg</td>\n      <td>1</td>\n      <td>A young man hanging over the side of a boat  w...</td>\n    </tr>\n    <tr>\n      <th>158912</th>\n      <td>998845445.jpg</td>\n      <td>2</td>\n      <td>A man is leaning off of the side of a blue and...</td>\n    </tr>\n    <tr>\n      <th>158913</th>\n      <td>998845445.jpg</td>\n      <td>3</td>\n      <td>A man riding a small boat in a harbor  with fo...</td>\n    </tr>\n    <tr>\n      <th>158914</th>\n      <td>998845445.jpg</td>\n      <td>4</td>\n      <td>A man on a moored blue and white boat with hil...</td>\n    </tr>\n  </tbody>\n</table>\n<p>78915 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"class Flickr30kImagesDataset(Dataset):\n    # Initialize CLIP model and preprocessing transforms\n    clip_model, _, clip_preprocess = create_model_and_transforms(\n        'ViT-bigG-14-quickgelu', pretrained='metaclip_fullcc', device='cuda'\n    )\n    clip_model.eval()  # Set to evaluation mode\n    \n    def __init__(self, image_dir: str, captions_df: pd.DataFrame, prefix_length: int, normalize_prefix=False):\n        \"\"\"\n        image_dir: Path to the directory containing images.\n        captions_df: DataFrame with 'image' and 'caption' columns.\n        prefix_length: Length of the prefix for the embeddings.\n        normalize_prefix: Whether to normalize the CLIP embeddings.\n        \"\"\"\n        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n        self.prefix_length = prefix_length\n        self.normalize_prefix = normalize_prefix\n\n        # Precompute tokens and CLIP embeddings\n        self.caption_tokens = []\n        self.caption2embedding = []\n        self.max_seq_len = 0\n        self._precompute_embeddings_and_tokens(image_dir, captions_df)\n\n    def _precompute_embeddings_and_tokens(self, image_dir, captions_df):\n        \"\"\"Precomputes tokens for captions and CLIP embeddings for images.\"\"\"\n        for idx, row in tqdm(captions_df.iterrows()):\n            image_filename = row['image']\n            caption = row['caption']\n\n            # Tokenize caption\n            tokens = torch.tensor(self.tokenizer.encode(caption), dtype=torch.int64)\n            self.caption_tokens.append(tokens)\n            self.max_seq_len = max(self.max_seq_len, tokens.shape[0])\n\n            # Process image to get CLIP embedding\n            # image_path = os.path.join(image_dir, image_filename)\n            # image = Image.open(image_path)\n            # image_tensor = self.clip_preprocess(image).unsqueeze(0)  # Add batch dimension\n\n            # with torch.no_grad():\n            #     embedding = self.clip_model.encode_image(image_tensor).squeeze(0)\n            # if self.normalize_prefix:\n            #     embedding = embedding / embedding.norm(2, -1)\n            # self.caption2embedding.append(embedding)\n\n            if idx % 5 == 0:\n            # Process image to get CLIP embedding\n                image_path = os.path.join(image_dir, image_filename)\n                image = Image.open(image_path)\n                image_tensor = Flickr30kImagesDataset.clip_preprocess(image).unsqueeze(0).to('cuda')  # Add batch dimension\n    \n                with torch.no_grad():\n                    embedding = Flickr30kImagesDataset.clip_model.encode_image(image_tensor).squeeze(0)\n                if self.normalize_prefix:\n                    embedding = embedding / embedding.norm(2, -1)\n                self.caption2embedding.append(embedding)\n            else:\n                self.caption2embedding.append(self.caption2embedding[-1].clone())\n\n    def pad_tokens(self, idx):\n        \"\"\"Pads tokens to the maximum sequence length and creates a mask.\"\"\"\n        tokens = self.caption_tokens[idx]\n        padding = self.max_seq_len - tokens.shape[0]\n        if padding > 0:\n            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n            self.caption_tokens[idx] = tokens\n        elif padding < 0:\n            tokens = tokens[:self.max_seq_len]\n            self.caption_tokens[idx] = tokens\n        mask = tokens.ge(0)  # Mask is zero where we are out of sequence\n        tokens[~mask] = 0\n        mask = mask.float()\n        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # Add prefix mask\n        return tokens, mask\n\n    def __len__(self):\n        return len(self.caption_tokens)\n\n    def __getitem__(self, idx):\n        tokens, mask = self.pad_tokens(idx)\n        prefix_embedding = self.caption2embedding[idx]\n        return tokens, mask, prefix_embedding\n\n    def to_pickle(self, pickle_path: str):\n        \"\"\"Saves the dataset information, including caption tokens, embeddings, and parameters, to a pickle file.\"\"\"\n        with open(pickle_path, 'wb') as f:\n            pickle.dump({\n                'caption_tokens': self.caption_tokens,\n                'caption2embedding': self.caption2embedding,\n                'max_seq_len': self.max_seq_len,\n                'prefix_length': self.prefix_length,\n                'normalize_prefix': self.normalize_prefix\n            }, f)\n\n    @classmethod\n    def from_pickle(cls, pickle_path: str):\n        \"\"\"Loads the dataset information, including caption tokens, embeddings, and parameters, from a pickle file.\"\"\"\n        with open(pickle_path, 'rb') as f:\n            data = pickle.load(f)\n        dataset = cls.__new__(cls)  # Create an uninitialized instance\n        dataset.caption_tokens = data['caption_tokens']\n        dataset.caption2embedding = data['caption2embedding']\n        dataset.max_seq_len = data['max_seq_len']\n        dataset.prefix_length = data['prefix_length']\n        dataset.normalize_prefix = data['normalize_prefix']\n        return dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:32:41.102064Z","iopub.execute_input":"2024-12-24T18:32:41.102370Z","iopub.status.idle":"2024-12-24T18:33:08.589804Z","shell.execute_reply.started":"2024-12-24T18:32:41.102347Z","shell.execute_reply":"2024-12-24T18:33:08.588489Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-c9f34102204c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mFlickr30kImagesDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Initialize CLIP model and preprocessing transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     clip_model, _, clip_preprocess = create_model_and_transforms(\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m'ViT-bigG-14-quickgelu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'metaclip_fullcc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n","\u001b[0;32m<ipython-input-31-c9f34102204c>\u001b[0m in \u001b[0;36mFlickr30kImagesDataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mFlickr30kImagesDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Initialize CLIP model and preprocessing transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     clip_model, _, clip_preprocess = create_model_and_transforms(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;34m'ViT-bigG-14-quickgelu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'metaclip_fullcc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/open_clip/factory.py\u001b[0m in \u001b[0;36mcreate_model_and_transforms\u001b[0;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, image_mean, image_std, image_interpolation, image_resize_mode, aug_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, load_weights_only, **model_kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     )\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     model = create_model(\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mpretrained\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/open_clip/factory.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, force_preprocess_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained, load_weights_only, **model_kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0mpretrained_loaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1172\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1158\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     )\n\u001b[0;32m-> 1160\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1161\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 6072 has 14.73 GiB memory in use. Of the allocated memory 14.42 GiB is allocated by PyTorch, and 190.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 6072 has 14.73 GiB memory in use. Of the allocated memory 14.42 GiB is allocated by PyTorch, and 190.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":31},{"cell_type":"code","source":"def merge(first: Flickr30kImagesDataset, second: Flickr30kImagesDataset):\n    dataset = Flickr30kImagesDataset.__new__(Flickr30kImagesDataset)  # Create an uninitialized instance\n    dataset.caption_tokens = first.caption_tokens + second.caption_tokens\n    dataset.caption2embedding = first.caption2embedding + second.caption2embedding\n    dataset.max_seq_len = max(first.max_seq_len, second.max_seq_len)\n    dataset.prefix_length = first.prefix_length\n    dataset.normalize_prefix = first.normalize_prefix\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:23:15.138380Z","iopub.execute_input":"2024-12-24T18:23:15.138725Z","iopub.status.idle":"2024-12-24T18:23:15.143340Z","shell.execute_reply.started":"2024-12-24T18:23:15.138699Z","shell.execute_reply":"2024-12-24T18:23:15.142348Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Макс\ndataset1 = Flickr30kImagesDataset(\n    IMAGE_PATH,\n    df1,\n    5,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:19:29.092562Z","iopub.execute_input":"2024-12-24T18:19:29.092841Z","iopub.status.idle":"2024-12-24T18:19:31.145748Z","shell.execute_reply.started":"2024-12-24T18:19:29.092811Z","shell.execute_reply":"2024-12-24T18:19:31.144897Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"252b5ea2aeac43b6871ee630b9c154b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"007d76c3d7094b59ba60d7b348c650b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a87a67550234e0eae071de9272bcac4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f87edca41c74413b8ce9abd110fa974c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae3782b367cd40f7a2393c490bb6422c"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n10it [00:01,  8.80it/s]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Илюха\n# dataset2 = Flickr30kImagesDataset(\n#     IMAGE_PATH,\n#     df2,\n#     5,\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:19:31.146672Z","iopub.execute_input":"2024-12-24T18:19:31.146917Z","iopub.status.idle":"2024-12-24T18:19:31.980245Z","shell.execute_reply.started":"2024-12-24T18:19:31.146896Z","shell.execute_reply":"2024-12-24T18:19:31.979367Z"}},"outputs":[{"name":"stderr","text":"10it [00:00, 14.71it/s]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"dataset1.to_pickle(\"Flickr30kImagesDatasetSave1\") # Макс","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:19:31.981167Z","iopub.execute_input":"2024-12-24T18:19:31.981424Z","iopub.status.idle":"2024-12-24T18:19:32.378179Z","shell.execute_reply.started":"2024-12-24T18:19:31.981401Z","shell.execute_reply":"2024-12-24T18:19:32.377497Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# dataset2.to_pickle(\"Flickr30kImagesDatasetSave2\") # Илюха","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:19:32.379075Z","iopub.execute_input":"2024-12-24T18:19:32.379398Z","iopub.status.idle":"2024-12-24T18:19:32.398219Z","shell.execute_reply.started":"2024-12-24T18:19:32.379367Z","shell.execute_reply":"2024-12-24T18:19:32.397318Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Теперь в Output есть файл, скачай его","metadata":{}},{"cell_type":"markdown","source":"# А дальше уже слияние, не запускаем пока","metadata":{}},{"cell_type":"code","source":"# check1 = Flickr30kImagesDataset.from_pickle(\"/kaggle/working/Flickr30kImagesDatasetSave1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:20:35.707749Z","iopub.execute_input":"2024-12-24T18:20:35.708085Z","iopub.status.idle":"2024-12-24T18:20:35.717130Z","shell.execute_reply.started":"2024-12-24T18:20:35.708057Z","shell.execute_reply":"2024-12-24T18:20:35.716440Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# check2 = Flickr30kImagesDataset.from_pickle(\"/kaggle/working/Flickr30kImagesDatasetSave2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:20:36.436438Z","iopub.execute_input":"2024-12-24T18:20:36.436839Z","iopub.status.idle":"2024-12-24T18:20:36.447261Z","shell.execute_reply.started":"2024-12-24T18:20:36.436812Z","shell.execute_reply":"2024-12-24T18:20:36.446550Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# final = merge(check1, check2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T18:23:18.193655Z","iopub.execute_input":"2024-12-24T18:23:18.193942Z","iopub.status.idle":"2024-12-24T18:23:18.198277Z","shell.execute_reply.started":"2024-12-24T18:23:18.193920Z","shell.execute_reply":"2024-12-24T18:23:18.197324Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# final.to_pickle(\"Flickr30kImagesDatasetSaveFinal\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}